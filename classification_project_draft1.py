# -*- coding: utf-8 -*-
"""Classification_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oNFQbmMSleYoPNwdYItqWl96njFSFocd

# PART 1: EXPLORATORY DATA ANALYSIS

Importing the the libraries that shall be used for the project. Will be updated as the project progresses
"""

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt

"""Uploading the original data file and loading it in a pandas dataframe for analysis"""

file = "/content/accepted_2007_to_2018Q4.csv"
raw_data = pd.read_csv(file, low_memory = False)

"""Printing the head of the dataframe to look at the basic format and understand the attributes"""

raw_data.head(5)

raw_data_shape = raw_data.shape
raw_data_shape

"""The data set contains a total of 2260701 rows under 151 attributes

Out of a total of 151 features, there are some features that are not of interest for the purpose of this project. Some of these features are not available to the investor at the time of investment and hence cannot be taken into account for classifying the investment. We have the metadata for the datafile from lending club that needs to be inspected to search for features that are actually available to the investor and consequently only these can be considered in the feature set for the classification. 

Loading the meta data into a pandas dataframe to understand the relevant features
"""

meta_data = pd.read_excel("/content/LCDataDictionary.xlsx", sheet_name = 'browseNotes')
print(meta_data.shape)
meta_data.head()

"""There are a total of 122 features of interest out of the 151 attributes in the original dataset. However, the names of the features are different in the metadata file comapred to that in the dataset. We need to match the features in the two files and keep only the features that are common to the two. This can be done either manually(time consuming) or with a few codes using regular expression. """

raw_data.columns = raw_data.columns.str.lower() # Converting the column names to all lower case
def data_col_match(raw_data, meta_data):
  ''' this function takes in a dataframe and a metadata associated with it and matches the column names in the dataframe to it's name present in the metadata file.
   After the matching is done, it returns the dmatching column names'''
  met_col_arr = meta_data['BrowseNotesFile'].dropna().values #adding the feature names in metadata as a list
  met_col_lower = list(map(lambda x: x.lower().strip(), met_col_arr)) #converting all the feature names in metadata into lowercases and storing them in a list
   
  #Doing the same for the column names of the raw_data
  raw_data_col = list(raw_data.columns.values)
  raw_col_lower = list(map(lambda x: x.lower().strip(), raw_data_col))

  #even after converting to lower case, there are still some mismatches due to spelling differences in the two files
  #This has to be manually changed in either of the two files to find the right meaningful matches
  match_cols = list(np.intersect1d(raw_col_lower, met_col_lower))
  return np.array((match_cols, raw_col_lower, met_col_lower))

match_cols = data_col_match(raw_data, meta_data)[0]

#71 features found a match in both the files. These shall be retained in addition to some others that are common but misspelled in either of the files
#Manual work to rename the column name in either of the files
raw_col_lower = data_col_match(raw_data, meta_data)[1]
met_col_lower = data_col_match(raw_data, meta_data)[2]

np.setdiff1d(raw_col_lower, met_col_lower)

np.setdiff1d(met_col_lower, raw_col_lower)

right_cols = ['acc_now_delinq', 'acc_open_past_24mths', 'addr_state',
       'annual_inc', 'bc_open_to_buy', 'bc_util', 'delinq_2yrs',
       'delinq_amnt', 'earliest_cr_line', 'emp_length',
       'fico_range_high', 'fico_range_low',
       'funded_amnt', 'home_ownership', 'initial_list_status',
       'inq_last_6mths', 'int_rate', 'last_credit_pull_d', 'loan_amnt',
       'member_id', 'mort_acc','mths_since_last_delinq',
       'mths_since_last_record', 'mths_since_recent_bc',
       'mths_since_recent_bc_dlq', 'mths_since_recent_inq', 'mths_since_recent_revol_delinq',
       'percent_bc_gt_75', 'pub_rec',
       'revol_bal', 'revol_util', 'sub_grade', 'total_acc', 
       'total_bal_ex_mort', 'total_bc_limit','verification_status_joint', 'loan_status']

total_cols = match_cols + right_cols
#total_cols

data_with_features = raw_data[total_cols].copy()
data_with_features_shape = data_with_features.shape

"""Finally we have the dataset with corresponding column matches in the data dictionary(meta data). The next step is to analyze this dataframe on a high level to understand parameters like mean, na values, etc

Using the describe function to get an overview of various attributes(mean, max, min and total count(excluding Nulls))
"""

data_with_features.describe()

"""Checking the percent of null values for each feature to take futher decision whether to consider them for the analysis or not."""

null_per_feature = (data_with_features.isnull().sum()/data_with_features_shape[0]) * 100
null_per_feature

"""Some of the features have a very high number of null values in terms of percentages. Dropping the attributes with over 50% nulls"""

data_without_null = pd.DataFrame()
for acol in data_with_features.columns:
  if(data_with_features[acol].isnull().sum()/data_with_features_shape[0] < 0.5 ):
    data_without_null[acol] = data_with_features[acol]

data_without_null.head(2)

"""After this filtering, a total of 84 features are left for analysis"""

data_without_null.describe()

"""Checking the loan repayment status for the entire data file(category vs count)"""

loan_status_values = pd.DataFrame(data_without_null['loan_status'].value_counts())
loan_status_values

"""Plotting a bar plot of the loan repayment status vs the corresponding count"""

loan_status_values.plot.bar()
plt.title("Loan Repayent Status")
plt.xlabel("Loan Repayment Status")
plt.ylabel("Total Count")

"""From the plot, it is clear that most of the accepted loans have either been paid off or are in current status or have been charged off. There are very few instances of loan default or late/in grace period. Hence these are the rarer class and are of more interest to us. There is a massive class imbalance in or dataset that we need to deal with. """

